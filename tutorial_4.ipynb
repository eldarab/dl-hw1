{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4\n",
    "\n",
    "* Implementation of Backpropagation on toy example\n",
    "* Vanishing\\exploding gradients demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example \n",
    "\n",
    "In this example we will build and train a neural-net for binary classification using only basic tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description:\n",
    "The network consists of 2 linear layers followed by a Sigmoid activation. The loss function is binary-cross-entropy.\n",
    "<a href=\"https://drive.google.com/uc?export=view&id=1Q8ucKs76yeiloZd-LgwNUw50BRJy2xSq\">\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=1Q8ucKs76yeiloZd-LgwNUw50BRJy2xSq\"\n",
    "    style=\"width: 400px; max-width: 100%; height: auto\"\n",
    "    title=\"Click for the larger version.\" />\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N = $ batch size <br>\n",
    "$X \\in \\mathbb{R}^{\\text{N} \\times 2}$\n",
    "\n",
    "$ W_1 \\in \\mathbb{R}^{2 \\times \\text{hidden}}$\n",
    "\n",
    "$ W_2 \\in \\mathbb{R}^{\\text{hidden} \\times 1}$\n",
    "\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=0}^{N} y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i} )$$\n",
    "\n",
    "##### Forward pass:\n",
    "$Z_1 = XW_1 + b_1$ $ \\quad \\quad \\left[Z_1 \\in \\mathbb{R}^{\\text{N} \\times \\text{hidden}}\\right]$<br>\n",
    "$H = \\sigma(Z_1)$<br>\n",
    "$Z_2 = HW_2 + b_2 \\quad \\quad \\left[Z_2 \\in \\mathbb{R}^{\\text{N} \\times 1}\\right]$<br>\n",
    "$\\hat{Y} = \\sigma(Z_2)$<br>\n",
    "\n",
    "##### Backward pass:\n",
    "We are interested in $\\frac{\\partial L}{\\partial W_1}$, $\\frac{\\partial L}{\\partial b_1}$, $\\frac{\\partial L}{\\partial W_2}$ and $\\frac{\\partial L}{\\partial b_2}$\n",
    "\n",
    "---------\n",
    "$\\large \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial H} \\frac{\\partial H}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial W_1}$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial H} \\frac{\\partial H}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial b_1}$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial W_2}$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial b_2}$\n",
    "\n",
    "---------\n",
    "\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial \\hat{Y}} = -\\frac{1}{N}\\left(\\frac{Y}{\\hat{Y}}-\\frac{1-Y}{1-\\hat{Y}}\\right) = \\frac{1}{N}\\frac{\\hat{Y}-Y}{\\hat{Y}(1-\\hat{Y})}$ $\\quad \\quad \\left[\\frac{\\partial L}{\\partial \\hat{Y}} \\in \\mathbb{R}^{\\text{N} \\times 1}\\right] $\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial \\hat{Y}}(\\sigma(Z_2)(1-\\sigma(Z_2)))$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial W_2} = H^T \\frac{\\partial L}{\\partial Z_2}$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial b_2} = (\\frac{\\partial L}{\\partial Z_2})^T1$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial H} = \\frac{\\partial L}{\\partial Z_2}W_2^T $\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial H}(\\sigma(Z_1)(1-\\sigma(Z_1)))$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial W_1} = (\\frac{\\partial L}{\\partial Z_1})^TX$\n",
    "\n",
    "$\\large \\frac{\\partial L}{\\partial b_1} = (\\frac{\\partial L}{\\partial Z_1})^T1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt  \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
      "         0.1332],\n",
      "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739, 0.2666,\n",
      "         0.6274],\n",
      "        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053, 0.2695, 0.3588, 0.1994, 0.5472,\n",
      "         0.0062],\n",
      "        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376, 0.8090, 0.5779, 0.9040, 0.5547,\n",
      "         0.3423],\n",
      "        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539,\n",
      "         0.1952],\n",
      "        [0.0050, 0.3068, 0.1165, 0.9103, 0.6440, 0.7071, 0.6581, 0.4913, 0.8913,\n",
      "         0.1447],\n",
      "        [0.5315, 0.1587, 0.6542, 0.3278, 0.6532, 0.3958, 0.9147, 0.2036, 0.2018,\n",
      "         0.2018],\n",
      "        [0.9497, 0.6666, 0.9811, 0.0874, 0.0041, 0.1088, 0.1637, 0.7025, 0.6790,\n",
      "         0.9155],\n",
      "        [0.2418, 0.1591, 0.7653, 0.2979, 0.8035, 0.3813, 0.7860, 0.1115, 0.2477,\n",
      "         0.6524],\n",
      "        [0.6057, 0.3725, 0.7980, 0.8399, 0.1374, 0.2331, 0.9578, 0.3313, 0.3227,\n",
      "         0.0162],\n",
      "        [0.2137, 0.6249, 0.4340, 0.1371, 0.5117, 0.1585, 0.0758, 0.2247, 0.0624,\n",
      "         0.1816],\n",
      "        [0.9998, 0.5944, 0.6541, 0.0337, 0.1716, 0.3336, 0.5782, 0.0600, 0.2846,\n",
      "         0.2007],\n",
      "        [0.5014, 0.3139, 0.4654, 0.1612, 0.1568, 0.2083, 0.3289, 0.1054, 0.9192,\n",
      "         0.4008],\n",
      "        [0.9302, 0.6558, 0.0766, 0.8460, 0.3624, 0.3083, 0.0850, 0.0029, 0.6431,\n",
      "         0.3908],\n",
      "        [0.6947, 0.0897, 0.8712, 0.1330, 0.4137, 0.6044, 0.7581, 0.9037, 0.9555,\n",
      "         0.1035],\n",
      "        [0.6258, 0.2849, 0.4452, 0.1258, 0.9554, 0.1330, 0.7672, 0.6757, 0.6625,\n",
      "         0.2297],\n",
      "        [0.9545, 0.6099, 0.5643, 0.0594, 0.7099, 0.4250, 0.2709, 0.9295, 0.6115,\n",
      "         0.2234],\n",
      "        [0.2469, 0.4761, 0.7792, 0.3722, 0.2147, 0.3288, 0.1265, 0.6783, 0.8870,\n",
      "         0.0293],\n",
      "        [0.6161, 0.7583, 0.5907, 0.3219, 0.7610, 0.7628, 0.6870, 0.4121, 0.3676,\n",
      "         0.5535],\n",
      "        [0.4117, 0.3510, 0.8196, 0.9297, 0.4505, 0.3881, 0.5073, 0.4701, 0.6202,\n",
      "         0.6401],\n",
      "        [0.0459, 0.3155, 0.9211, 0.6948, 0.4751, 0.1985, 0.1941, 0.0521, 0.3370,\n",
      "         0.6689],\n",
      "        [0.8188, 0.7308, 0.0580, 0.1993, 0.4211, 0.9837, 0.5723, 0.3705, 0.7069,\n",
      "         0.3096],\n",
      "        [0.1764, 0.8649, 0.2726, 0.3998, 0.0026, 0.8346, 0.8788, 0.6822, 0.1514,\n",
      "         0.0065],\n",
      "        [0.0939, 0.8729, 0.7401, 0.9208, 0.7619, 0.6265, 0.4951, 0.1197, 0.0716,\n",
      "         0.0323],\n",
      "        [0.7047, 0.2545, 0.3994, 0.2122, 0.4089, 0.1481, 0.1733, 0.6659, 0.3514,\n",
      "         0.8087],\n",
      "        [0.3396, 0.1332, 0.4118, 0.2576, 0.3470, 0.0240, 0.7797, 0.1519, 0.7513,\n",
      "         0.7269],\n",
      "        [0.8572, 0.1165, 0.8596, 0.2636, 0.6855, 0.9696, 0.4295, 0.4961, 0.3849,\n",
      "         0.0825],\n",
      "        [0.7400, 0.0036, 0.8104, 0.8741, 0.9729, 0.3821, 0.0892, 0.6124, 0.7762,\n",
      "         0.0023],\n",
      "        [0.3865, 0.2003, 0.4563, 0.2539, 0.2956, 0.3413, 0.0248, 0.9103, 0.9192,\n",
      "         0.4216],\n",
      "        [0.4431, 0.2959, 0.0485, 0.0134, 0.6858, 0.2255, 0.1786, 0.4610, 0.3335,\n",
      "         0.3382],\n",
      "        [0.5161, 0.3939, 0.3278, 0.2606, 0.0931, 0.9193, 0.2999, 0.6325, 0.3265,\n",
      "         0.5406],\n",
      "        [0.9662, 0.7304, 0.0667, 0.6985, 0.9746, 0.6315, 0.8352, 0.9929, 0.4234,\n",
      "         0.6038],\n",
      "        [0.1525, 0.3970, 0.8703, 0.7563, 0.1836, 0.0991, 0.1583, 0.0066, 0.1142,\n",
      "         0.3764],\n",
      "        [0.8374, 0.5837, 0.1197, 0.0989, 0.7487, 0.1281, 0.4384, 0.7399, 0.2686,\n",
      "         0.4455],\n",
      "        [0.4565, 0.3817, 0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585, 0.1642,\n",
      "         0.6212],\n",
      "        [0.6378, 0.7740, 0.8801, 0.7784, 0.0042, 0.5443, 0.8029, 0.4538, 0.2054,\n",
      "         0.9767],\n",
      "        [0.3130, 0.2153, 0.0492, 0.5223, 0.7216, 0.6107, 0.5989, 0.1208, 0.0331,\n",
      "         0.5088],\n",
      "        [0.9559, 0.7885, 0.2089, 0.4351, 0.1314, 0.2588, 0.5905, 0.7723, 0.9142,\n",
      "         0.0409],\n",
      "        [0.8343, 0.1474, 0.6872, 0.9231, 0.5070, 0.9549, 0.0740, 0.3090, 0.7916,\n",
      "         0.3911],\n",
      "        [0.3976, 0.2916, 0.8447, 0.7453, 0.6602, 0.2190, 0.0941, 0.5541, 0.6481,\n",
      "         0.2691],\n",
      "        [0.3601, 0.8377, 0.5398, 0.5226, 0.3769, 0.0472, 0.0299, 0.2610, 0.2458,\n",
      "         0.6558],\n",
      "        [0.3544, 0.3044, 0.9767, 0.6742, 0.8565, 0.2579, 0.2958, 0.6838, 0.1669,\n",
      "         0.1731],\n",
      "        [0.4759, 0.3171, 0.1252, 0.7966, 0.9021, 0.5811, 0.4129, 0.0369, 0.3179,\n",
      "         0.6273],\n",
      "        [0.7358, 0.4368, 0.3023, 0.7786, 0.1018, 0.8160, 0.3060, 0.5077, 0.4012,\n",
      "         0.5606],\n",
      "        [0.3489, 0.8636, 0.4870, 0.8903, 0.9807, 0.2564, 0.1352, 0.9012, 0.8918,\n",
      "         0.1182],\n",
      "        [0.4613, 0.0069, 0.0907, 0.5966, 0.6330, 0.6060, 0.3639, 0.9613, 0.5715,\n",
      "         0.2050],\n",
      "        [0.4717, 0.6201, 0.6751, 0.1465, 0.6874, 0.2446, 0.0845, 0.2269, 0.9822,\n",
      "         0.9274],\n",
      "        [0.9477, 0.7935, 0.8777, 0.4331, 0.2249, 0.7498, 0.2409, 0.1626, 0.3403,\n",
      "         0.6049],\n",
      "        [0.7574, 0.3058, 0.2057, 0.5674, 0.2053, 0.1745, 0.7606, 0.4160, 0.9569,\n",
      "         0.9864],\n",
      "        [0.6496, 0.6721, 0.6151, 0.5078, 0.4636, 0.5069, 0.6867, 0.9649, 0.3704,\n",
      "         0.2886],\n",
      "        [0.3789, 0.2584, 0.5850, 0.8732, 0.8910, 0.7296, 0.1320, 0.2316, 0.3901,\n",
      "         0.4078],\n",
      "        [0.5411, 0.0410, 0.6556, 0.1186, 0.1836, 0.0843, 0.9357, 0.0265, 0.8772,\n",
      "         0.4832],\n",
      "        [0.4419, 0.8127, 0.4538, 0.8136, 0.8615, 0.0659, 0.6924, 0.5944, 0.6075,\n",
      "         0.5730],\n",
      "        [0.6368, 0.2595, 0.4360, 0.9751, 0.8359, 0.4812, 0.0297, 0.5219, 0.1595,\n",
      "         0.9066],\n",
      "        [0.1965, 0.4639, 0.3890, 0.5890, 0.9705, 0.5475, 0.7896, 0.8881, 0.9037,\n",
      "         0.3273],\n",
      "        [0.3882, 0.7410, 0.3636, 0.7341, 0.3908, 0.1609, 0.7035, 0.5767, 0.7229,\n",
      "         0.9967],\n",
      "        [0.8414, 0.9740, 0.5268, 0.0699, 0.1492, 0.1894, 0.0594, 0.2494, 0.0397,\n",
      "         0.0387],\n",
      "        [0.2012, 0.0071, 0.1931, 0.6907, 0.9170, 0.3513, 0.3546, 0.7670, 0.2533,\n",
      "         0.2636],\n",
      "        [0.8081, 0.0643, 0.5611, 0.9417, 0.5857, 0.6360, 0.2088, 0.4931, 0.5275,\n",
      "         0.6227],\n",
      "        [0.6943, 0.9345, 0.1184, 0.5150, 0.2502, 0.1045, 0.4600, 0.0599, 0.8489,\n",
      "         0.5579],\n",
      "        [0.2305, 0.7613, 0.0268, 0.3066, 0.4026, 0.0751, 0.1821, 0.4184, 0.8794,\n",
      "         0.9828],\n",
      "        [0.8181, 0.2014, 0.1729, 0.9363, 0.6769, 0.5133, 0.5677, 0.0982, 0.3331,\n",
      "         0.9813],\n",
      "        [0.3767, 0.4749, 0.0848, 0.2203, 0.4898, 0.1894, 0.4380, 0.7035, 0.0109,\n",
      "         0.6485],\n",
      "        [0.1694, 0.2560, 0.6920, 0.8976, 0.3633, 0.2947, 0.0479, 0.2422, 0.0622,\n",
      "         0.3856],\n",
      "        [0.6020, 0.0316, 0.9366, 0.8137, 0.0105, 0.2612, 0.6631, 0.3973, 0.4455,\n",
      "         0.2742],\n",
      "        [0.9016, 0.2205, 0.9146, 0.5323, 0.6005, 0.8901, 0.4176, 0.2153, 0.4191,\n",
      "         0.9055],\n",
      "        [0.1290, 0.6135, 0.0086, 0.7622, 0.6847, 0.5212, 0.7146, 0.5006, 0.7767,\n",
      "         0.1042],\n",
      "        [0.4266, 0.7218, 0.9979, 0.7547, 0.1364, 0.8845, 0.3885, 0.3932, 0.0455,\n",
      "         0.4213],\n",
      "        [0.8537, 0.5697, 0.2088, 0.6539, 0.3397, 0.9565, 0.0660, 0.3421, 0.0172,\n",
      "         0.3031],\n",
      "        [0.6576, 0.9813, 0.5840, 0.9902, 0.5978, 0.7888, 0.9008, 0.9180, 0.2201,\n",
      "         0.9597],\n",
      "        [0.8029, 0.2662, 0.2614, 0.0806, 0.6256, 0.0947, 0.7112, 0.6579, 0.0656,\n",
      "         0.6363],\n",
      "        [0.4593, 0.7284, 0.7869, 0.0029, 0.9585, 0.9193, 0.6989, 0.0430, 0.3214,\n",
      "         0.3551],\n",
      "        [0.3715, 0.7820, 0.6818, 0.8961, 0.3127, 0.6683, 0.6779, 0.0837, 0.0150,\n",
      "         0.2406],\n",
      "        [0.8423, 0.0293, 0.0648, 0.7801, 0.7698, 0.9112, 0.1225, 0.1341, 0.7565,\n",
      "         0.9348],\n",
      "        [0.7992, 0.5783, 0.6648, 0.9746, 0.1774, 0.2730, 0.8497, 0.1579, 0.2243,\n",
      "         0.8650],\n",
      "        [0.6578, 0.6615, 0.2881, 0.4931, 0.9576, 0.1999, 0.5039, 0.7378, 0.1548,\n",
      "         0.9856],\n",
      "        [0.2502, 0.3799, 0.3647, 0.1742, 0.0094, 0.7819, 0.6328, 0.0317, 0.1782,\n",
      "         0.9942],\n",
      "        [0.6911, 0.7006, 0.2009, 0.2808, 0.4245, 0.4086, 0.1574, 0.5412, 0.5497,\n",
      "         0.4367],\n",
      "        [0.5693, 0.3018, 0.6301, 0.6886, 0.2366, 0.0042, 0.7617, 0.6193, 0.2457,\n",
      "         0.9819],\n",
      "        [0.2739, 0.8379, 0.7537, 0.0808, 0.8225, 0.0403, 0.2230, 0.4166, 0.1630,\n",
      "         0.9885],\n",
      "        [0.3997, 0.6986, 0.0535, 0.7878, 0.3446, 0.1197, 0.5731, 0.7422, 0.9327,\n",
      "         0.1946],\n",
      "        [0.2539, 0.5961, 0.6356, 0.6922, 0.7744, 0.3866, 0.7778, 0.8686, 0.3694,\n",
      "         0.8557],\n",
      "        [0.7443, 0.9410, 0.2159, 0.2531, 0.3554, 0.5254, 0.8001, 0.2146, 0.7503,\n",
      "         0.3208],\n",
      "        [0.8021, 0.4763, 0.0620, 0.2249, 0.1381, 0.7480, 0.1647, 0.4583, 0.6079,\n",
      "         0.2258],\n",
      "        [0.6442, 0.0118, 0.1422, 0.0469, 0.3488, 0.3179, 0.5716, 0.4075, 0.7350,\n",
      "         0.9584],\n",
      "        [0.6794, 0.3030, 0.0318, 0.6811, 0.2523, 0.7544, 0.8342, 0.6929, 0.9692,\n",
      "         0.9749],\n",
      "        [0.6059, 0.1357, 0.9467, 0.2628, 0.2638, 0.9184, 0.8874, 0.6511, 0.5313,\n",
      "         0.0794],\n",
      "        [0.4481, 0.9796, 0.6273, 0.5428, 0.3962, 0.3256, 0.7980, 0.5308, 0.8253,\n",
      "         0.4115],\n",
      "        [0.7185, 0.7064, 0.5797, 0.8142, 0.8133, 0.9635, 0.8844, 0.3722, 0.0767,\n",
      "         0.5914],\n",
      "        [0.4956, 0.3696, 0.4163, 0.5235, 0.8648, 0.6559, 0.3225, 0.2944, 0.3762,\n",
      "         0.3067],\n",
      "        [0.9496, 0.7648, 0.9515, 0.5016, 0.6008, 0.6734, 0.0267, 0.5446, 0.4666,\n",
      "         0.2197],\n",
      "        [0.1120, 0.9426, 0.9065, 0.7317, 0.9771, 0.2971, 0.4136, 0.6893, 0.4174,\n",
      "         0.4019],\n",
      "        [0.0867, 0.6343, 0.1978, 0.5182, 0.9875, 0.3461, 0.3424, 0.8017, 0.3162,\n",
      "         0.4571],\n",
      "        [0.9669, 0.2950, 0.1423, 0.2202, 0.3614, 0.2628, 0.2405, 0.7020, 0.5850,\n",
      "         0.3400],\n",
      "        [0.1115, 0.3426, 0.2890, 0.3373, 0.0489, 0.6077, 0.1326, 0.1106, 0.0915,\n",
      "         0.7087],\n",
      "        [0.1990, 0.2936, 0.8919, 0.7655, 0.7867, 0.0252, 0.1415, 0.3112, 0.9130,\n",
      "         0.5512],\n",
      "        [0.1261, 0.5031, 0.1117, 0.3905, 0.3625, 0.9328, 0.6549, 0.4128, 0.5845,\n",
      "         0.3557],\n",
      "        [0.6965, 0.6978, 0.6343, 0.3051, 0.9266, 0.4278, 0.3053, 0.8132, 0.9075,\n",
      "         0.9976],\n",
      "        [0.6481, 0.3296, 0.7539, 0.9290, 0.0096, 0.4381, 0.1590, 0.5932, 0.7068,\n",
      "         0.3967],\n",
      "        [0.4582, 0.7251, 0.4160, 0.0801, 0.9001, 0.2483, 0.4451, 0.5472, 0.4700,\n",
      "         0.0297]])\n",
      "tensor([9, 6, 8, 2, 6, 2, 5, 0, 1, 3, 0, 0, 7, 4, 6, 3, 9, 2, 7, 6, 0, 0, 1, 7,\n",
      "        7, 3, 1, 8, 6, 4, 2, 7, 4, 3, 2, 7, 7, 4, 2, 6, 8, 7, 0, 4, 3, 0, 2, 8,\n",
      "        0, 6, 3, 2, 4, 4, 2, 0, 7, 4, 9, 5, 8, 8, 8, 6, 7, 6, 2, 2, 0, 8, 2, 5,\n",
      "        3, 2, 6, 6, 3, 4, 9, 4, 0, 6, 6, 1, 3, 4, 4, 3, 7, 5, 9, 0, 1, 6, 1, 3,\n",
      "        5, 5, 9, 6])\n",
      "myloss tensor(2.3436)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3436)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 100\n",
    "x = torch.rand(batch_size,10)\n",
    "y = torch.randint(low=0, high=10, size=(batch_size, ))\n",
    "print(\"myloss\", cross_entropy_loss(x, y))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##### Utils functions \n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "def sigmoidPrime(s):\n",
    "    # derivative of sigmoid\n",
    "    # s: sigmoid output\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(t):\n",
    "    return torch.div(torch.exp(t) - torch.exp(-t), torch.exp(t) + torch.exp(-t))\n",
    "\n",
    "def tanhPrime(t):\n",
    "    # derivative of tanh\n",
    "    # t: tanh output\n",
    "    return 1 - t*t\n",
    "\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1, keepdim=True)\n",
    "\n",
    "def cross_entropy_loss(x, y):\n",
    "    loss = 0\n",
    "    assert x.shape[0] == y.shape[0] # make sure batch sizes are the same\n",
    "    nll = - torch.log(softmax(x))\n",
    "    for prob, true_class in zip(nll, y):\n",
    "        loss += prob[true_class]\n",
    "    return loss / x.shape[0]\n",
    "\n",
    "def cross_entropy_loss_prime(x, y):\n",
    "    ret = softmax(x).squeeze()\n",
    "    for i, yi in enumerate(y):\n",
    "        ret[i][yi] -= 1\n",
    "    return ret.sum(0) / x.shape[0]\n",
    "        \n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_size=784, hidden_size=32, output_size=10):\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = torch.zeros(self.hidden_size)\n",
    "        \n",
    "        self.W2 = torch.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = torch.zeros(self.output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z1 = torch.matmul(X, self.W1) + self.b1\n",
    "        self.h = tanh(self.z1)\n",
    "        self.z2 = torch.matmul(self.h, self.W2) + self.b2\n",
    "        return sigmoid(self.z2)\n",
    "    \n",
    "    def backward(self, X, y, y_hat, lr=.1):\n",
    "        batch_size = y.size(0)\n",
    "        dl_dz2 = (1/batch_size)*(y_hat - y)  \n",
    "\n",
    "        dl_dh = torch.matmul(dl_dz2, torch.t(self.W2))\n",
    "        dl_dz1 = dl_dh * sigmoidPrime(self.h)\n",
    "        \n",
    "        self.W1 -= lr*torch.matmul(torch.t(X), dl_dz1)\n",
    "        self.b1 -= lr*torch.matmul(torch.t(dl_dz1), torch.ones(batch_size))\n",
    "        self.W2 -= lr*torch.matmul(torch.t(self.h), dl_dz2)\n",
    "        self.b2 -= lr*torch.matmul(torch.t(dl_dz2), torch.ones(batch_size))\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# MNIST Dataset (Images and Labels)\n",
    "\n",
    "batch_size = 100\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1, 28*28))])\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transform,\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transform)\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3631)\n"
     ]
    }
   ],
   "source": [
    "net = Neural_Network()\n",
    "for batch_ndx, batch in enumerate(train_loader):\n",
    "    output = net.forward(batch[0]).squeeze()\n",
    "    loss = cross_entropy_loss(output, batch[1])\n",
    "    print(loss)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}